{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9222f6fd15b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m## Import the PyTorch modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  IDS594: Machine Learning Application with Python\n",
    " \n",
    "  - Filename: train.py\n",
    "  - Reference: Problem 1 \n",
    "  - Comments:\n",
    "      Your job at this code is to understand high-level structure of the assignment, \n",
    "      verifying how to load the data, and \n",
    "\"\"\"\n",
    "\n",
    "## For pretty print of tensors.\n",
    "## Must be located at the first line except the comments.\n",
    "from __future__ import print_function\n",
    "\n",
    "## Import the basic modules.\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "## Import the PyTorch modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## Import the custom module to load the CIFAR-10 dataset.\n",
    "## (This is not a built-in module, but included in cifar10.py)\n",
    "from cifar10 import CIFAR10\n",
    "\n",
    "## You are supposed to implement the following four source codes:\n",
    "## {softmax.py, twolayernn.py, convnet.py, mymodel.py}.\n",
    "import models.softmax \n",
    "import models.twolayernn\n",
    "import models.convnet\n",
    "import models.mymodel\n",
    "\n",
    "## Initilize a command-line option parser.\n",
    "parser = argparse.ArgumentParser(description='CIFAR-10 Example')\n",
    "\n",
    "## Add a list of command-line options that users can specify.\n",
    "## Shell scripts (.sh) files for specifying the proper options are provided. \n",
    "parser.add_argument('--lr', type=float, metavar='LR', help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, metavar='M', help='SGD momentum')\n",
    "parser.add_argument('--weight-decay', type=float, default=0.0, help='Weight decay hyperparameter')\n",
    "parser.add_argument('--batch-size', type=int, metavar='N', help='input batch size for training')\n",
    "parser.add_argument('--epochs', type=int, metavar='N', help='number of epochs to train')\n",
    "parser.add_argument('--model', choices=['softmax', 'convnet', 'twolayernn', 'mymodel'], help='which model to train/evaluate')\n",
    "parser.add_argument('--hidden-dim', type=int, help='number of hidden features/activations')\n",
    "parser.add_argument('--kernel-size', type=int, help='size of convolution kernels/filters')\n",
    "\n",
    "## Add more command-line options for other configurations.\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True, help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 1)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N', help='number of batches between logging train status')\n",
    "parser.add_argument('--cifar10-dir', default='data', help='directory that contains cifar-10-batches-py/ (downloaded automatically if necessary)')\n",
    "\n",
    "## Parse the command-line option.\n",
    "args = parser.parse_args()\n",
    "\n",
    "## CUDA will be supported only when user wants and the machine has GPU devices.\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "## Change the random seed.\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "## Set the device-specific arguments if CUDA is available.\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "## Initialize size information for CIFAR10 dataset\n",
    "## CIFAR-10 consists of 32x32 color images with 3 channels.\n",
    "## Each image is one of the ten classes: {airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck}.\n",
    "im_size = (3, 32, 32)   \n",
    "n_classes = 10\n",
    "\n",
    "## Normalize each image by subtracdting the mean color and divde by standard deviation.\n",
    "## For convenience, per channel mean color and standard deviation are provided.\n",
    "cifar10_mean_color = [0.49131522, 0.48209435, 0.44646862]\n",
    "cifar10_std_color = [0.01897398, 0.03039277, 0.03872553]\n",
    "\n",
    "## Define a standardized transform by using PyTorch's transforms module.\n",
    "## Just one preparation can be used for every split of dataset.\n",
    "transform = transforms.Compose([\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(cifar10_mean_color, cifar10_std_color),\n",
    "            ])\n",
    "\n",
    "## Load training, validation, and test data separately.\n",
    "## (CIFAR10 class inherits PyTorch Dataset class to load datafile on disk)\n",
    "## Apply the normalizing transform uniformly across three dataset.\n",
    "train_dataset = CIFAR10(args.cifar10_dir, split='train', download=True, transform=transform)\n",
    "val_dataset = CIFAR10(args.cifar10_dir, split='val', download=True, transform=transform)\n",
    "test_dataset = CIFAR10(args.cifar10_dir, split='test', download=True, transform=transform)\n",
    "\n",
    "## DataLoaders provide various ways to get batches of examples.\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "## Load the proper neural network model.\n",
    "if args.model == 'softmax':\n",
    "    # Problem 2 (no hidden layer, input -> output)\n",
    "    model = models.softmax.Softmax(im_size, n_classes)\n",
    "elif args.model == 'twolayernn':\n",
    "    # Problem 3 (one hidden layer, input -> hidden layer -> output)\n",
    "    model = models.twolayernn.TwoLayerNN(im_size, args.hidden_dim, n_classes)\n",
    "elif args.model == 'convnet':\n",
    "    # Problem 4 (multiple hidden layers, input -> hidden layers -> output)\n",
    "    model = models.convnet.CNN(im_size, args.hidden_dim, args.kernel_size, n_classes)\n",
    "elif args.model == 'mymodel':\n",
    "    # Problem 5 (multiple hidden layers, input -> hidden layers -> output)\n",
    "    model = models.mymodel.MyModel(im_size, args.hidden_dim, args.kernel_size, n_classes)\n",
    "else:\n",
    "    raise Exception('Unknown model {}'.format(args.model))\n",
    "\n",
    "## Deinfe the loss function as cross-entropy.\n",
    "## This is the softmax loss function (i.e., multiclass classification).\n",
    "criterion = functional.cross_entropy\n",
    "\n",
    "## Activate CUDA if specified and available.\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(list(model.parameters()), lr=args.lr, momentum=args.momentum)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "## Function: train the model just one iteration.\n",
    "def train(epoch):\n",
    "    # Recall model is a class that inherits nn.Module that we learned in the class.\n",
    "    # This puts the model in train mode as opposed to eval mode, so it knows which one to use.\n",
    "    model.train()\n",
    "    \n",
    "    # For each batch of training images,\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Read images and their target labels in the current batch.\n",
    "        images, targets = Variable(batch[0]), Variable(batch[1])\n",
    "        \n",
    "        # Load the current training example in the CUDA core if available.\n",
    "        if args.cuda:\n",
    "            images, targets = images.cuda(), targets.cuda()\n",
    "\n",
    "        model.zero_grad() # pytorch accumulates gradients, making them zero for each minibatch\n",
    "        \n",
    "        #forward pass\n",
    "        out = model(images)\n",
    "        \n",
    "        #backward pass \n",
    "        L = criterion(out,targets) #calculate loss\n",
    "        L.backward() # calculate gradients\n",
    "        optimizer.step() # make an update step\n",
    "        \n",
    "        # Print out the loss and accuracy on the first 4 batches of the validation set.\n",
    "        # You can adjust the printing frequency by changing --log-interval option in the command-line.\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            # Compute the average validation loss and accuracy.\n",
    "            val_loss, val_acc = evaluate('val', n_batches=4)\n",
    "            \n",
    "            # Compute the training loss.\n",
    "            train_loss = L.data[0]\n",
    "            \n",
    "            # Compute the number of examples in this batch.\n",
    "            examples_this_epoch = batch_idx * len(images)\n",
    "\n",
    "            # Compute the progress rate in terms of the batch.\n",
    "            epoch_progress = 100. * batch_idx / len(train_loader)\n",
    "\n",
    "            # Print out the training loss, validation loss, and accuracy with epoch information.\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t'\n",
    "                  'Train Loss: {:.6f}\\tVal Loss: {:.6f}\\tVal Acc: {}'.format(\n",
    "                epoch, examples_this_epoch, len(train_loader.dataset),\n",
    "                epoch_progress, train_loss, val_loss, val_acc))\n",
    "\n",
    "## Function: evaluate the learned model on either validation or test data.\n",
    "def evaluate(split, verbose=False, n_batches=None):\n",
    "    # Recall model is a class that inherits nn.Module that we learned in the class.\n",
    "    # This puts the model in eval mode as opposed to train mode, so it knows which one to use.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize cumulative loss and the number of correctly predicted examples.  \n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    n_examples = 0\n",
    "\n",
    "    # Load the correct dataset between validation and test data based on the split option.\n",
    "    if split == 'val':\n",
    "        loader = val_loader\n",
    "    elif split == 'test':\n",
    "        loader = test_loader\n",
    "\n",
    "    # For each batch in the loaded dataset,\n",
    "    for batch_i, batch in enumerate(loader):        \n",
    "        data, target = batch\n",
    "        # Load the current training example in the CUDA core if available.\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        # Read images and their target labels in the current batch.\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        # Measure the output results given the data.\n",
    "        output = model(data)\n",
    "\n",
    "        # Accumulate the loss by comparing the predicted output and the true target labels.\n",
    "        loss += criterion(output, target, size_average=False).data[0]\n",
    "        \n",
    "        # Predict the class by finding the argmax of the log-probabilities among all classes.\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        # Add the number of correct classifications in each class.\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "        # Keep track of the total number of predictions.\n",
    "        n_examples += pred.size(0)\n",
    "\n",
    "        # Skip the rest of evaluation if the number of batches exceed the n_batches.\n",
    "        if n_batches and (batch_i >= n_batches):\n",
    "            break\n",
    "\n",
    "    # Compute the average loss per example.\n",
    "    loss /= n_examples\n",
    "\n",
    "    # Compute the average accuracy in terms of percentile.\n",
    "    acc = 100. * correct / n_examples\n",
    "\n",
    "    # If verbose is True, then print out the average loss and accuracy.\n",
    "    if verbose:        \n",
    "        print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            split, loss, correct, n_examples, acc))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "## Train the model one epoch at a time.\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "## Evaluate the model on the test data and print out the average loss and accuracy.\n",
    "## Note that you should use every batch for evaluating on test data rather than just the first four batches.\n",
    "evaluate('test', verbose=True)\n",
    "\n",
    "## Save the model (architecture and weights)\n",
    "torch.save(model, args.model + '.pt')\n",
    "\n",
    "\"\"\"\n",
    "# Later you can call torch.load(file) to re-load the trained model into python\n",
    "# See http://pytorch.org/docs/master/notes/serialization.html for more details\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
